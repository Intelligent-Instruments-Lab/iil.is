---
layout: news
date: "2024-06-24"
slug: "icad2024"
title: "Miguel Crozzoli presenting two papers at ICAD 2024"
description: "The International Conference on Auditory Display 2024 is being held in Troy, NY, USA"
featured: true
highlight_image: "news/icad2024.jpg"
---

<script>
    import CaptionedImage from "../../components/Images/CaptionedImage.svelte"
</script>

<CaptionedImage
src="news/icad2024.jpg"
alt="International Conference on Auditory Display 2024 is being held in Troy, NY, USA"
caption="International Conference on Auditory Display 2024 is being held in Troy, NY, USA"
/>

# International Conference on Auditory Display 2024

[Miguel Crozzoli](/people#miguel-crozzoli) will be in Troy, New York for ICAD 2024 to present two papers from the lab.
For full details, visit the [ICAD 2024 website](https://icad2024.icad.org).

## Title

[Miguel Crozzoli](/people#miguel-crozzoli) and [Thor Magnusson](/people#thor-magnusson)

### Abstract

Abstract

## Artificial Life in Integrated Interactive Sonification and Visualisation: Initial Experiments with a Python-Based Workflow

[Jack Armitage](/people#jack-armitage), [Miguel Crozzoli](/people#miguel-crozzoli) & [Daniel Jones](https://www.erase.net/)

Read the full paper: [PDF](http://iil.is/pdf/2024_icad_armitage_et_al_alife.pdf).

### Abstract

Multimodal displays that combine interaction, sonification, visualisation and perhaps other modalities, are seeing increased interest from researchers seeking to take advantage of cross-modal perception, by increasing display bandwidth and expanding affordances. To support researchers and designers, many new tools are being proposed that aim to consolidate these broad feature sets into Python libraries, due to Python's extensive ecosystem that in particular encompasses the domain of artificial intelligence (AI). Artificial life (ALife) is a domain of AI that is seeing renewed interest, and in this work we share initial experiments exploring its potential in interactive sonification, through the combination of two new Python libraries, [Tölvera](/research/tolvera) and [SignalFlow](http://signalflow.dev). Tölvera is a library for composing self-organising systems, with integrated open sound control, interactive machine learning, and computer vision, and SignalFlow is a sound synthesis framework that enables real-time interaction with an audio signal processing graph via standard Python syntax and data types. We demonstrate how these two tools integrate, and the first author reports on usage in creative coding and artistic performance. So far we have found it useful to consider ALife as affording synthetic behaviour as a display modality, making use of human perception of complex, collective and emergent dynamics. In addition, we think ALife also implies a broader perspective on interaction in multimodal display, blurring the lines between data, agent and observer. Based on our experiences, we offer possible future research directions for tool designers and researchers.

