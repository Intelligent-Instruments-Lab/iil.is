---
layout: news
date: "2022-06-09"
title: "Robin joins the IIL, turning weather data into music"
description: "On Robin's project this summer"
featured: true
---

<script>
  import CaptionedImage from "../../components/Images/CaptionedImage.svelte"
</script>

We're happy to have Robin Morabito join us this summer for an internship supported by the Student Innovation Fund. He'll be working on a very exciting project, transforming weather data into sound! Here you can read all about him and his project, in his own words...

<CaptionedImage
  src="news/robin_device.jpeg"
  alt="A young man holding an unusual device. Yellow shelving system in the background."
  caption="Robin Morabito designed an interactive sonification instrument."/>

I'm Robin, recently graduated from Composition - New Media at the Iceland University of the Arts. I've played music since I was a kid, starting with guitar and slowly extending to violin, vocal studies, computer music and sound engineering. These last few years I got increasingly interested in sonification, which is an awesome way to make music out of raw data and learn what information those data contain. 

I did some experiments with real-time Icelandic weather data during the past couple of years, culminating in two pieces: Veðurhornið (an interactive performance using weather data and Facebook reactions in real-time to generate an extended French horn score) and Veðurgítarar (a Max patch using real-time weather data to produce generative music). 

Veðurhornið
<iframe width="560" height="315" src="https://www.youtube.com/watch?v=B3oXS7LfJ2I&t=24s" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Veðurgítarar
<iframe width="560" height="315" src="https://www.youtube.com/watch?v=vr4iij3tnBw&t=9s" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

In my last piece, presented on May the 17th in Reykjavík (Dynjandi hall), I used a Bela sensor board to embed a generative Pure Data music patch inside of a sculpture suspended in the middle of the air; audience could interact with the object and the soundscape produced via 6 sensors placed on the body of the sculpture. This interactive sound installation, obscurely titled "The only object they could retrieve from Earth's lost civilization", anticipates the project I hope to keep developing at the Intelligent Instrument Lab: an interactive sonification instrument, capable of adapting to the person using it and to the kind of dataset chosen. 

The Only Object They Could Retrieve from Earth's Lost Civilization
<iframe width="560" height="315" src="https://www.youtube.com/watch?v=nAro0fELOv8&t=8s" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Sonification is becoming more and more popular to portray large multidimensional datasets, which can be perfectly represented with music. While our technology becomes increasingly complex, we have to find instruments to relate to the data that we produce and study: that's why sonification is so exciting, and why I wanted to spend three months in the lab learning more about it while learning how to perfect this process, to ultimately provide the community of people interested with additional resources for augmented sonification and sonification tools.
