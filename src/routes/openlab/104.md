---
layout: openlab
edition: 104
theme: "Bryan Jacobs: Everything Everywhere and Other Mechanized Sound"
description: "Bryan Jacobs will discuss robotic instruments used in his performances and compositions."
date: "2025-05-23"
highlight_image: "openlabs/vlns3.jpg"
---

<script>
    import CaptionedImage from "../../components/Images/CaptionedImage.svelte"
</script>

<CaptionedImage
src="openlabs/vlns3.jpg"
alt="Photo of a performance."
caption=""/>

In this open lab we will have three amazing guests presenting their work: Brian Jacobs, who is working with [Ensemble Adapter](https://ensemble-adapter.de/) on a performance in [Tjarnarbio](https://www.tjarnarbio.is/syningar/kolkrabbinn) on Thursday next week, and Chris Kiefer and Andrea Martelloni, who are performing in [Mengi](https://mengi.net/events/2025/5/24/chris-kiefer-amp-andrea-martelloni) this Saturday.

** Where: Edda, room 218 ** (Map [here](https://maps.app.goo.gl/NpAThcrc73VttzGi6))

** When: Friday, May 23rd, 3-5 pm **

**[Facebook event](https://fb.me/e/7Mo0kSqSO)**

****

# Everything Everywhere
For this next open lab Bryan Jacobs will discuss robotic instruments used in his performances Everything Everywhere, Other Mechanized Sound and other compositions.

Everything Everywhere is a one-hour music performance by Ensemble Adapter and Ensemble Pamplemousse, that has been in development since summer 2023. The core members of both groups - Natacha Diels, Bryan Jacobs, Gunnhildur Einarsdóttir and Matthias Engler - act as a creative collective. This evening length multimedia performance combines mechanical instruments, video, story-telling, and musical performance to present an abstract construction of a musical "monster".


### Bryan Jacobs

Composer, performer, and sound artist, Bryan Jacobs’ work focuses on interactions between live performers, mechanical instruments and computers. His pieces are often theatrical in nature, pitting blabber-mouthed fanciful showoffs against timid reluctants. The sounds are playfully organized and many times mimic patterns found in human dialogue. Hand-build electromechanical instruments controlled by microcontrollers bridge acoustic and electroacoutic sound worlds. These instruments live dual lives as time-based concert works and non-time-based gallery works.

---------

# Towards an ecosystem of Musically Embodied Machine Learning

This seminar introduces the concept of Musically Embodied Machine Learning (MEML), a new way to integrate machine learning deeply into musical practice. Today’s AI tools often feel disembodied and disconnected from musicians’ embodied creativity, forcing performers to work through laptops and abstract interfaces. We argue for a different vision: what if the instrument itself could be the interface to machine learning, allowing artists to train, tune, and evolve models in real time, as part of their natural performance flow? MEML instruments are self-contained musical devices that make machine learning tangible, expressive, and responsive. Our goal is to create a vibrant ecosystem where musicians, rather than adapting to distant AI systems, directly shape and dialogue with them through their instruments—reclaiming agency, spontaneity, and artistry in the age of machine learning.

### Chris Kiefer

Chris Kiefer is a musician and musical instrument designer specializing in musician-computer interaction, physical computing, and machine learning. His research focuses on feedback musicianship, live coding, and embodied machine learning, combining personal performance practice with participatory design. Recent projects include The Nalima, a membrane-based feedback instrument, the Feedback Cello, and uSEQ, a eurorack module for live coding. He co-led the AHRC-funded Feedback Musicianship Network and holds an AHRC Fellowship for the Musically Embodied Machine Learning project.

### Andrea Martelloni

Andrea Martelloni is a multi-instrumentalist, signal processing engineer, and researcher in machine learning for musical interaction. His PhD at the Centre for Doctoral Training in Artificial Intelligence and Music (Queen Mary University of London) explores real-time deep learning for virtuoso musical performance. He is now a postdoctoral researcher on the Musically Embodied Machine Learning project at the University of Sussex, developing machine learning tools for embedded musical devices.

---
<br>

We look forward to seeing you!

Free entry, accessible to all.
