---
layout: openlab
edition: 106
theme: "Øyvind Brandtsegg and Daniel Formo: Realtime Rhythm Analysis"
description: "How to work flexibly with expressive rhythms in the computer."
date: "2025-06-10"
highlight_image: "openlabs/Open_lab_106.png"
---

<script>
    import CaptionedImage from "../../components/Images/CaptionedImage.svelte"
</script>

<CaptionedImage
src="openlabs/Open_lab_106.png"
alt="Rhythm analysis."
caption=""/>



** Where: Veröld, room 103 ** (Map [here](https://maps.app.goo.gl/Jr61r6v7ompEhAXz7))

** When: Tuesday, June 10th, 3-5 pm **

**[Facebook event](https://fb.me/e/1LJuTigFw4)**

****

# Realtime Rhythm Analysis

Realtime rhythm analysis, representation and generation - how to work flexibly with expressive rhythms in the computer.

Pitch and frequency can be analyzed and represented with relative ease, but rhythm is a harder problem. Expressive rhythm analysis is a challenge not yet solved although research has been ongoing since the early 1980's. Methods are usually based on assumptions from western (art) music, like rhythmic hierarchy, pulse and meter. We will present a prototype of a new model for rhythm analysis and representation based on perception of time ratios between events. The new method requires only a small set of events to produce meaningful results. It is based on maintaining several simultaneous/parallel theories of integer relationships of delta times between events in a time series. On order to select between the different possible representations, we evaluate representation complexity and deviation. In our presentation we will also go into difficulties with quantization errors, and reconciliation of representation across rhythmic phrases.
<br>

The work in our project generally relates to computer assisted music creation, utilizing lean datasets in custom made intelligent and generative models. We will investigate if this allows a more nuanced control and better affordances regarding intentionality, when compared with deep learning models based on large data sets. We propose that current machine learning models are often unsatisfactory for realizing a creative intention. This is in part due to their basis in a statistical average over a large set of training data. Furthermore, the environmental impact of computing with large data sets is significant and ethically questionable. We note, also within the computing disciplines, that there is a growing interest to utilize intelligent generative models that rely less on huge datasets, but focus on smaller and more compact datasets. For our purposes, these more compact models could potentially allow for personalization, parametrization and precise creative control.
<br>

## Øyvind Brandtsegg
Øyvind Brandtsegg is a composer and performer working in the fields of computer improvisation and sound installations. He has a deep interest in developing new instruments and audio processing methods for artistic purposes, and he has contributed novel extensions to both granular synthesis, feedback systems, and live convolution techniques. Brandtsegg has participated on more than 25 music albums in a variety of genres. Since 2010 he is a professor of music technology at NTNU, Trondheim, Norway.
<br>

## Daniel Formo
Daniel Formo is a keyboard player, improviser and sound artist, working within a broad range of music from improvised and written contemporary music, to jazz and popular genres, as well as electroacoustic music and electronic art. His work takes a special interest in the relationship between language and music, instrument building and the relationship between the acoustic and digital, virtual and real. Formo is an associate professor of music technology at NTNU, Trondheim, Norway.
<br>


---
<br>



We look forward to seeing you!

Free entry, accessible to all.
