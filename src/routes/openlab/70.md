---
layout: openlab
edition: 70
theme: "Sahar Ghaderi and Manuel Cherep"
description: "Digital Models and Synth Programming"
date: "2023-11-24"
highlight_image: "openlabs/me-2.jpg"
---

<script>
    import CaptionedImage from "../../components/Images/CaptionedImage.svelte"
</script>


<CaptionedImage
    src="openlabs/Sahar photo.jpeg"
    alt="Black and white photo of a woman wiht a snowy background" 
    caption="Sahar Ghaderi"/>

# Sahar Ghaderi:

The use and practice of infographic restitutions in the field of built-up material vestiges has always been an important foundation in research and scientific experiments. In this presentation I will talk about the role of digital models from the excavation to the representation of research results. I will present a part of my experience from my PhD thesis which was about a multidisciplinary work of architecture, archaeology, and IT specialists. 

Bio: 
 
Sahar Ghaderi is an architect Ph.D. with an extensive experience in architectural firms in Iceland, France, and Iran and has been in charge of many projects focalizing on architecture, urbanism, landscape, and cultural heritage. Her works are characterized by research-based design, interaction, and communication in a multidisciplinary and international environment.  
  
Current research of Sahar emphasizes the use of Infrastructure of information and data processing in architectural and urban projects.

Links:

https://www.lhi.is/en/user/19583 

https://www.sprint-studio.com/ 

    
<CaptionedImage
    src="openlabs/me-2.jpg"
    alt="Blue darkness, a person puts their ear into a tank full of water" 
    caption="Manuel Cherep"/>

# Manuel Cherep: Creative Text-to-Audio Generation via Synthesizer Programming

Sound designers have long harnessed the power of abstraction to distill and highlight the semantic essence of real-world auditory phenomena, akin to how simple sketches can vividly convey visual concepts. However, current neural audio synthesis methods lean heavily towards capturing acoustic realism. We introduce a novel method centered on meaningful abstraction. Our approach, leveraging a virtual modular synthesizer, takes a text prompt and iteratively refines the synthesizer's parameters to produce sounds with high semantic alignment, as predicted by a pretrained audio-language model. This approach is training-free and lightweight, making it suitable for consumer hardware. Additionally, it offers a fully interpretable parameter space, allowing users to intuitively adjust generated sounds to their tastes as well as inspect and understand their attributes. Our results underscore the distinctiveness of our synthesized sounds compared with both real recordings and state-of-the-art generative models.

Bio:

Manuel is a multidisciplinary graduate researcher and Fulbright Scholar at the MIT Media Lab exploring the intersection of machine learning, cognitive & affective science, and animal behavior with a keen interest in perception, accessibility, emergence, and audio. He has a background in mathematics and computer science, having received an MSc in Data Science at EPFL and worked at Logitech as an AI Engineer. Both academically and artistically, his work underscores the significance of fostering greater attention to animals and nature, to our bodies, and to each other.

Links

https://mcherep.github.io/

https://github.com/PapayaResearch/synthax

https://ctag.media.mit.edu/
